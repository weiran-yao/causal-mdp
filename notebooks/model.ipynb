{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Probablistic Physics Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_scatter import scatter_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fully_connected_edge_index(n_objects):\n",
    "    n_relations  = n_objects * (n_objects - 1)\n",
    "    edge_index = torch.zeros((2, n_relations), dtype=torch.long)\n",
    "    count = 0\n",
    "    for i in range(n_objects):\n",
    "        for j in range(n_objects):\n",
    "            if(i != j):\n",
    "                edge_index[0, count] = i\n",
    "                edge_index[1, count] = j\n",
    "                count += 1\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallSimulationDataset(Dataset):\n",
    "    def __init__(self, root, raw_folder_name, processed_folder_name, use_cuda=False):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.raw_folder = os.path.join(root, raw_folder_name)\n",
    "        self.processed_folder = os.path.join(root, processed_folder_name)\n",
    "        if not os.path.exists(self.raw_folder):\n",
    "            os.mkdir(self.raw_folder)\n",
    "        if not os.path.exists(self.processed_folder):\n",
    "            os.mkdir(self.processed_folder)\n",
    "        self.filenames = [fn.split(\".\")[0] for fn in os.listdir(self.raw_folder) if \"npz\" in fn]\n",
    "        self._process_raw_files()\n",
    "        if use_cuda:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        \n",
    "    def _process_raw_files(self):\n",
    "        for filename in self.filenames:\n",
    "            pt_fn = os.path.join(self.processed_folder, filename+'.pt')\n",
    "            if not os.path.exists(pt_fn):\n",
    "                raw_file_path = os.path.join(self.raw_folder, filename+'.npz')\n",
    "                batch = np.load(raw_file_path)\n",
    "                batch_x = torch.Tensor(batch[\"X\"])\n",
    "                batch_y = torch.Tensor(batch[\"Y\"])\n",
    "                n_objects = batch_x.shape[2]\n",
    "                edge_index = create_fully_connected_edge_index(n_objects)\n",
    "                torch.save((batch_x, batch_y, edge_index), pt_fn)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        pt_fn = os.path.join(self.processed_folder, filename+'.pt')\n",
    "        batch = torch.load(pt_fn)\n",
    "        batch = [item.to(self.device) for item in batch]\n",
    "        return batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/media/data/pymunk_dataset/\"\n",
    "raw_folder_name = \"raw\"\n",
    "processed_folder_name = \"processed\"\n",
    "dataset = BallSimulationDataset(root, raw_folder_name, processed_folder_name, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y, edge_index = dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeModel(nn.Module):\n",
    "    def __init__(self, node_dims, edge_dims, u_dims, hidden_size=32):\n",
    "        super().__init__()\n",
    "        input_size = 2*node_dims\n",
    "        self.edge_mlp = Seq(Lin(input_size, hidden_size), ReLU(), Lin(hidden_size, edge_dims))\n",
    "\n",
    "    def forward(self, src, dest):\n",
    "        # source, target: [E, F_x], where E is the number of edges.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u], where B is the number of graphs.\n",
    "        # batch: [E] with max entry B - 1.\n",
    "        out = torch.cat([src, dest], dim=-1)\n",
    "        return self.edge_mlp(out)\n",
    "\n",
    "class NodeModel(torch.nn.Module):\n",
    "    def __init__(self, node_dims, edge_dims, u_dims, hidden_size=32):\n",
    "        super().__init__()\n",
    "        mlp_1_input_size = node_dims+edge_dims\n",
    "        self.node_mlp_1 = Seq(Lin(mlp_1_input_size, hidden_size), ReLU(), Lin(hidden_size, hidden_size))\n",
    "        mlp_2_input_size = node_dims+hidden_size\n",
    "        self.node_mlp_2 = Seq(Lin(mlp_2_input_size, hidden_size), ReLU(), Lin(hidden_size, node_dims))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: [bs, N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [bs, E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        row, col = edge_index\n",
    "        out = torch.cat([x[:,row,:], edge_attr], dim=-1)\n",
    "        out = self.node_mlp_1(out)\n",
    "        out = scatter_mean(out, col, dim=1, dim_size=x.size(1))\n",
    "        out = torch.cat([x, out], dim=-1)\n",
    "        return self.node_mlp_2(out)\n",
    "\n",
    "class GlobalModel(torch.nn.Module):\n",
    "    def __init__(self, node_dims, u_dims, hidden_size=32):\n",
    "        super().__init__()\n",
    "        input_size = node_dims\n",
    "        self.global_mlp = Seq(Lin(input_size, hidden_size), ReLU(), Lin(hidden_size, u_dims))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        out = torch.mean(x, dim=1, keepdim=False)\n",
    "        #out = scatter_mean(x, batch, dim=0)\n",
    "        return self.global_mlp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLayer(torch.nn.Module):\n",
    "    \"\"\"A meta layer for building any kind of graph network, inspired by the\n",
    "    Relational Inductive Biases, Deep Learning, and Graph Networks\n",
    "    <https://arxiv.org/abs/1806.01261>`_ paper.\n",
    "\n",
    "    Args:\n",
    "        edge_model (Module, optional): A callable which updates a graph's edge\n",
    "            features based on its source and target node features, its current\n",
    "            edge features and its global features. (default: :obj:`None`)\n",
    "        node_model (Module, optional): A callable which updates a graph's node\n",
    "            features based on its current node features, its graph\n",
    "            connectivity, its edge features and its global features.\n",
    "            (default: :obj:`None`)\n",
    "        global_model (Module, optional): A callable which updates a graph's\n",
    "            global features based on its node features, its graph connectivity,\n",
    "            its edge features and its current global features.\n",
    "    \"\"\"\n",
    "    def __init__(self, edge_model=None, node_model=None, global_model=None):\n",
    "        super().__init__()\n",
    "        self.edge_model = edge_model\n",
    "        self.node_model = node_model\n",
    "        self.global_model = global_model\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for item in [self.node_model, self.edge_model, self.global_model]:\n",
    "            if hasattr(item, 'reset_parameters'):\n",
    "                item.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\"\"\"\n",
    "        row, col = edge_index\n",
    "        if self.edge_model is not None:\n",
    "            edge_attr = self.edge_model(x[:,row,:], x[:,col,:])\n",
    "        \n",
    "        if self.node_model is not None:\n",
    "            x = self.node_model(x, edge_index, edge_attr)\n",
    "        if self.global_model is not None:\n",
    "            u = self.global_model(x)\n",
    "        return x, edge_attr, u\n",
    "\n",
    "    def __repr__(self):\n",
    "        return ('{}(\\n'\n",
    "                '    edge_model={},\\n'\n",
    "                '    node_model={},\\n'\n",
    "                '    global_model={}\\n'\n",
    "                ')').format(self.__class__.__name__, self.edge_model,\n",
    "                            self.node_model, self.global_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNLSTMRecgnition(nn.Module):\n",
    "    def __init__(self, hidden_size=16, latent_size=2):\n",
    "        super().__init__()\n",
    "        ## GNN ##\n",
    "        self.node_dims = 7+2 #X+y\n",
    "        self.hidden_size=hidden_size\n",
    "        self.edge_dims=hidden_size\n",
    "        self.u_dims=hidden_size \n",
    "        edge_model = EdgeModel(self.node_dims, self.edge_dims, self.u_dims, self.hidden_size)\n",
    "        node_model = NodeModel(self.node_dims, self.edge_dims, self.u_dims, self.hidden_size)\n",
    "        global_model = GlobalModel(self.node_dims, self.u_dims, self.hidden_size)\n",
    "        self.op = MetaLayer(edge_model, node_model, global_model)\n",
    "        ## LSTM ##\n",
    "        self.edge_rnn = nn.LSTM(self.edge_dims, self.hidden_size, batch_first=True)\n",
    "        self.node_rnn = nn.LSTM(self.node_dims, self.hidden_size, batch_first=True)\n",
    "        self.global_rnn = nn.LSTM(self.u_dims, self.hidden_size, batch_first=True)\n",
    "        ## Linear ##\n",
    "        self.edge_fc_1 = nn.Linear(self.hidden_size, latent_size)\n",
    "        self.edge_fc_2 = nn.Linear(self.hidden_size, latent_size)\n",
    "        self.node_fc_1 = nn.Linear(self.hidden_size, latent_size)\n",
    "        self.node_fc_2 = nn.Linear(self.hidden_size, latent_size)\n",
    "        self.global_fc_1 = nn.Linear(self.hidden_size, latent_size)\n",
    "        self.global_fc_2 = nn.Linear(self.hidden_size, latent_size)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # define the forward computation on the latent z\n",
    "        # x shape: [n_experiments, steps, nodes, node_dims]\n",
    "        bs, steps, nodes, node_dims = x.size()\n",
    "        _, edges = edge_index.size()\n",
    "        x_reshape = x.view(-1, nodes, node_dims)\n",
    "        node_attr, edge_attr, global_attr = self.op(x_reshape, edge_index)\n",
    "        node_attr = node_attr.view(bs, steps, nodes, self.node_dims).permute(0,2,1,3)\n",
    "        edge_attr = edge_attr.view(bs, steps, edges, self.edge_dims).permute(0,2,1,3)\n",
    "        global_attr = global_attr.view(bs, steps, self.u_dims)\n",
    "        node_attr = node_attr.reshape(-1, steps, self.node_dims)\n",
    "        edge_attr = edge_attr.reshape(-1, steps, self.edge_dims)\n",
    "        # RNN forward\n",
    "        node_out, _ = self.node_rnn(node_attr)\n",
    "        edge_out, _ = self.edge_rnn(edge_attr)\n",
    "        global_out, _ = self.global_rnn(global_attr)\n",
    "        # get the outputs of the last time step\n",
    "        node_out  = node_out[:, -1, :]\n",
    "        edge_out = edge_out[:, -1, :]\n",
    "        global_out = global_out[:, -1, :]\n",
    "        # FC forward\n",
    "        z_node_mu = self.node_fc_1(node_out).view(bs, nodes, -1)\n",
    "        z_node_logvar = self.node_fc_2(node_out).view(bs, nodes, -1)\n",
    "        z_edge_mu = self.edge_fc_1(edge_out).view(bs, edges, -1)\n",
    "        z_edge_logvar = self.edge_fc_2(edge_out).view(bs, edges, -1)\n",
    "        z_global_mu = self.global_fc_1(global_out).view(bs, -1)\n",
    "        z_global_logvar = self.global_fc_2(global_out).view(bs, -1)\n",
    "        return [z_node_mu, z_node_logvar, z_edge_mu, z_edge_logvar, z_global_mu, z_global_logvar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x: [batch_size, n_relations, input_size]\n",
    "        Returns:\n",
    "            [batch_size, n_relations, output_size]\n",
    "        '''\n",
    "        batch_size, n_relations, input_size = x.size()\n",
    "        x = x.view(-1, input_size)\n",
    "        x = self.layers(x)\n",
    "        x = x.view(batch_size, n_relations, self.output_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 2), #speedX and speedY\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x: [batch_size, n_objects, input_size]\n",
    "        Returns:\n",
    "            [batch_size * n_objects, 2] speedX and speedY\n",
    "        '''\n",
    "        input_size = x.size(2)\n",
    "        x = x.view(-1, input_size)\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionNetworkGenerator(nn.Module):\n",
    "    def __init__(self, effect_dims=4, hidden_size=128, latent_size=2):\n",
    "        super().__init__()\n",
    "        # setup the two linear transformations used\n",
    "        self.node_dims = 7+latent_size #X+z_node\n",
    "        self.z_edge_dims = latent_size\n",
    "        self.z_u_dims = latent_size\n",
    "        self.effect_dims = effect_dims\n",
    "        self.hidden_size = hidden_size\n",
    "        self.relational_model = RelationalModel(2*self.node_dims + self.z_edge_dims, \n",
    "                                                self.effect_dims, self.hidden_size)\n",
    "        self.object_model     = DynamicsModel(self.node_dims+self.effect_dims+self.z_u_dims,\n",
    "                                              self.hidden_size)\n",
    "    def forward(self, x, z, edge_index):\n",
    "        # x shape: [bs, steps, nodes, node_dims]\n",
    "        bs, steps, nodes, node_dims = x.size()\n",
    "        sender_relations = F.one_hot(edge_index[0]).T.unsqueeze(0).repeat(bs*steps,1,1).type(torch.float)\n",
    "        receiver_relations = F.one_hot(edge_index[1]).T.unsqueeze(0).repeat(bs*steps,1,1).type(torch.float)\n",
    "        _, edges = edge_index.size()\n",
    "        row, col = edge_index\n",
    "        # define the forward computation on the latent z\n",
    "        z_node, z_edge, z_global = z\n",
    "        z_node = torch.unsqueeze(z_node, dim=1).repeat(1, steps, 1, 1)\n",
    "        z_edge = z_edge.repeat(steps, 1, 1)\n",
    "        z_global = torch.unsqueeze(z_global, dim=1).repeat(steps, nodes, 1)\n",
    "        x = torch.cat((x, z_node), dim=-1).view(bs*steps, nodes, -1)\n",
    "        senders   = sender_relations.permute(0, 2, 1).bmm(x)\n",
    "        receivers = receiver_relations.permute(0, 2, 1).bmm(x)\n",
    "        effects = self.relational_model(torch.cat([senders, receivers, z_edge], -1))\n",
    "        effect_receivers = receiver_relations.bmm(effects)\n",
    "        # predicted shape [bs*steps, nodes, 2]\n",
    "        predicted = self.object_model(torch.cat([x, z_global,effect_receivers], -1))\n",
    "        return predicted.view(bs, steps, nodes, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsVAE(nn.Module):\n",
    "    # by default our latent space is 50-dimensional\n",
    "    # and we use 400 hidden units\n",
    "    def __init__(self, z_dim=128, effect_dims=128, use_cuda=False):\n",
    "        super().__init__()\n",
    "        # create the encoder and decoder networks\n",
    "        self.encoder = GNNLSTMRecgnition(hidden_size=128, latent_size=z_dim)\n",
    "        self.decoder = InteractionNetworkGenerator(effect_dims=effect_dims, hidden_size=128, latent_size=z_dim)\n",
    "\n",
    "        if use_cuda:\n",
    "            # calling cuda() here will put all the parameters of\n",
    "            # the encoder and decoder networks into gpu memory\n",
    "            self.cuda()\n",
    "        self.use_cuda = use_cuda\n",
    "        self.z_dim = z_dim\n",
    "    \n",
    "    def encode(self, xy, edge_index):\n",
    "        return self.encoder(xy, edge_index)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, x, z, edge_index):\n",
    "        return self.decoder(x, z, edge_index)\n",
    "\n",
    "    def forward(self, x, y, edge_index):\n",
    "        xy = torch.cat((x, y), dim=-1)\n",
    "        self.z_stats = self.encode(xy, edge_index)\n",
    "        z_node_mu, z_node_logvar, z_edge_mu, z_edge_logvar, z_global_mu, z_global_logvar = self.z_stats\n",
    "        z_node = self.reparameterize(z_node_mu, z_node_logvar)\n",
    "        z_edge = self.reparameterize(z_edge_mu, z_edge_logvar)\n",
    "        z_global = self.reparameterize(z_global_mu, z_global_logvar)\n",
    "        z_sample = [z_node, z_edge, z_global]\n",
    "        return self.decode(x, z_sample, edge_index), self.z_stats\n",
    "    \n",
    "    def inference(self, x_test, edge_index):\n",
    "        # x_test shape [1, steps, n_objects, node_dims]\n",
    "        with torch.no_grad():\n",
    "            z_node_mu, z_node_logvar, z_edge_mu, z_edge_logvar, z_global_mu, z_global_logvar = self.z_stats\n",
    "            ez_node_logvar = torch.mean(z_node_logvar, dim=0)\n",
    "            ez_edge_mu = torch.mean(z_edge_mu, dim=0)\n",
    "            ez_edge_logvar = torch.mean(z_edge_logvar, dim=0)\n",
    "            ez_global_mu = torch.mean(z_global_mu, dim=0)\n",
    "            ez_global_logvar = torch.mean(z_global_logvar, dim=0)\n",
    "            z_node = self.reparameterize(ez_node_mu, ez_node_logvar).unsqueeze(0)\n",
    "            z_edge = self.reparameterize(ez_edge_mu, ez_edge_logvar).unsqueeze(0)\n",
    "            z_global = self.reparameterize(ez_global_mu, ez_global_logvar).unsqueeze(0)\n",
    "            z_sample = [z_node, z_edge, z_global]\n",
    "            pred = self.decode(x_test, z_sample, edge_index)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = PhysicsVAE(use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, use_cuda=False):\n",
    "    # Load dataset\n",
    "    root = \"/media/data/pymunk_dataset/\"\n",
    "    raw_folder_name = \"raw\"\n",
    "    processed_folder_name = \"processed\"\n",
    "    dataset = BallSimulationDataset(root, raw_folder_name, processed_folder_name, \n",
    "                                    use_cuda=use_cuda)\n",
    "    # Create model\n",
    "    model = PhysicsVAE(use_cuda=use_cuda)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    model.train()\n",
    "    # Training\n",
    "    for e_idx in range(epoch):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (batch_x, batch_y, edge_index) in enumerate(dataset):\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, z_stats = model(batch_x, batch_y, edge_index)\n",
    "            loss, reconstr_loss = loss_function(recon_batch, batch_y, z_stats)\n",
    "            loss.backward()\n",
    "            train_loss += reconstr_loss.item()\n",
    "            optimizer.step()\n",
    "        if e_idx % 50 == 0:\n",
    "            print(e_idx, train_loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, z_stats):\n",
    "    beta = 1\n",
    "    theta = 1e+20\n",
    "    MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    mmse = F.mse_loss(recon_x, x, reduction='mean')\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    z_node_mu, z_node_logvar, z_edge_mu, z_edge_logvar, z_global_mu, z_global_logvar = z_stats\n",
    "    KLD_node = -0.5 * torch.sum(1 + z_node_logvar - z_node_mu.pow(2) - z_node_logvar.exp())\n",
    "    KLD_edge = -0.5 * torch.sum(1 + z_edge_logvar - z_edge_mu.pow(2) - z_edge_logvar.exp())\n",
    "    KLD_global = -0.5 * torch.sum(1 + z_global_logvar - z_global_mu.pow(2) - z_global_logvar.exp())\n",
    "    KLD = KLD_node+KLD_edge+KLD_global\n",
    "    # latent discrepancy within batch\n",
    "    MSE_node = torch.sum(torch.var(z_node_mu, dim=0))+torch.sum(torch.var(z_node_logvar, dim=0))\n",
    "    MSE_edge = torch.sum(torch.var(z_edge_mu, dim=0))+torch.sum(torch.var(z_edge_logvar, dim=0))\n",
    "    MSE_global = torch.sum(torch.var(z_global_mu, dim=0))+torch.sum(torch.var(z_global_logvar, dim=0))\n",
    "    MSED = MSE_node+MSE_edge+MSE_global\n",
    "    return MSE + beta*KLD + theta*MSED, mmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 216.96520233154297\n",
      "50 33.0487003326416\n",
      "100 21.85648822784424\n",
      "150 16.11427593231201\n"
     ]
    }
   ],
   "source": [
    "model = train(200, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch15",
   "language": "python",
   "name": "torch15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
